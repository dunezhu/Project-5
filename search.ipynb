{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgRebn7qRvCmAKKI7pg/s2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIpVbJR7g-XD"
      },
      "outputs": [],
      "source": [
        "#Install the necessary libraries\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "!pip install python-docx\n",
        "\n",
        "# Import the libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Set the BASE URL you want to search\n",
        "url = ['https://www.website.suffix'] "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for url in url:\n",
        "    # send a GET request to the website\n",
        "    response = requests.get(url)\n",
        "\n",
        "# Parse the HTML content of the website\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# find all <a> tags and print their href\n",
        "for a in soup.find_all('a', href=True):\n",
        "    print(a['href']) "
      ],
      "metadata": {
        "id": "TrhDAfPY-4lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vet the URL List for errors\n",
        " \n",
        "# List of URLs to crawl \n",
        "urls = [\n",
        "\n",
        "#5th LAYER \n",
        "\n",
        "       ]\n",
        "\n",
        "for url in urls:\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        # Process the response content here\n",
        "        print(f\"Success: {url}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {url} - {str(e)}\") "
      ],
      "metadata": {
        "id": "2TaOB5wQRwMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Searching a URL LIST for links and nested pages\n",
        "from docx import Document\n",
        "from google.colab import files  \n",
        "\n",
        "# Set the subsequent LAYER URLs you want to crawl separated by a , \n",
        "websites = [\n",
        "\n",
        "#NTH LAYER\n",
        "\n",
        "           ]\n",
        "\n",
        "# Create a new Word document\n",
        "document = Document()\n",
        "\n",
        "# find all <a> tags and print their href\n",
        "for url in websites:\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    \n",
        "    print(f'Links found on {url}')\n",
        "    for link in soup.find_all('a'):\n",
        "        document.add_paragraph(link.get('href'))\n",
        "\n",
        "# Save the changes to a modified document\n",
        "document.save('modified.docx')\n",
        "\n",
        "# Download the modified Word file\n",
        "files.download('modified.docx')        "
      ],
      "metadata": {
        "id": "mklsqwVt1hCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORT 'modified.docx' TO USE IN 'search_list.ipynb'    "
      ],
      "metadata": {
        "id": "-OxphBuO-Tq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the URLs you want to crawl separated by a ,\n",
        "urls = [     \n",
        "\n",
        "#NTH LAYER\n",
        "\n",
        "       ] \n",
        "\n",
        "for url in urls:\n",
        "    # send a GET request to the website\n",
        "    response = requests.get(url)"
      ],
      "metadata": {
        "id": "O8G8XODqzEse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the location where keyword is found\n",
        "def search_keyword(urls, keyword):\n",
        "    for url in urls:\n",
        "        page = requests.get(url)\n",
        "        soup = BeautifulSoup(page.content, 'html.parser')\n",
        "        count = 0\n",
        "        for text in soup.find_all(text=True):\n",
        "            if keyword in text:\n",
        "                count +=1\n",
        "        if count:\n",
        "            print(f'Keyword found {count} times at {url}')\n",
        "        \n",
        "search_keyword(urls, 'Keyword')  "
      ],
      "metadata": {
        "id": "PerRXZyT-SnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the individual URL you want to search\n",
        "url = ['https://www.website.suffix/subject']  \n",
        " \n",
        "for url in url:\n",
        "    # send a GET request to the website\n",
        "    response = requests.get(url)"
      ],
      "metadata": {
        "id": "ivdngJFp-aFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Search for specific keywords\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "keywords = ['keyword']\n",
        "for keyword in keywords:\n",
        "    matches = soup.find_all(string=lambda text: keyword in text)\n",
        "    print(f'{len(matches)} matches found for keyword \"{keyword}\":')\n",
        "    for match in matches:\n",
        "        print(match)"
      ],
      "metadata": {
        "id": "NDQkBXTRQe_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find a specific 'kind' of tags on the 'specific page'\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "paragraphs = soup.find_all('p')\n",
        "\n",
        "# Set the keywords you want to search for\n",
        "keywords = ['']\n",
        "\n",
        "# Search for the keywords in each paragraph\n",
        "for p in paragraphs:\n",
        "  if any(keyword in p.text for keyword in keywords):\n",
        "    print(p.text)"
      ],
      "metadata": {
        "id": "MZApD2sm8MfX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}