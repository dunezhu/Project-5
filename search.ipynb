{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSgw3rqvCIAfaFE/6LKw05"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIpVbJR7g-XD"
      },
      "outputs": [],
      "source": [
        "#1 First, install the necessary libraries\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "\n",
        "# Import the libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Set the BASE URL you want to search\n",
        "url = ['https://en.wikipedia.org/wiki/Main_Page']  \n",
        " \n",
        "for url in url:\n",
        "    # send a GET request to the website\n",
        "    response = requests.get(url)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 Parse the HTML content of the website\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# find all <a> tags and print their href\n",
        "for a in soup.find_all('a', href=True):\n",
        "    print(a['href'])\n",
        "  \n",
        "#COPY THIS LIST TO 'search_list.ipynb'"
      ],
      "metadata": {
        "id": "onJ83IwK9iFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3 Searching a URL LIST for links and nested pages\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "# Set the 2nd LAYER URLs you want to crawl separated by a ,\n",
        "websites = [\n",
        "    \n",
        "'https://en.wikipedia.org/wiki/Special:Search', 'https://en.wikipedia.org/wiki/Help:Introduction'\n",
        "\n",
        "  ]\n",
        "\n",
        "# find all <a> tags and print their href\n",
        "for url in websites:\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    \n",
        "    print(f'Links found on {url}')\n",
        "    for link in soup.find_all('a'):\n",
        "        print(link.get('href'))\n"
      ],
      "metadata": {
        "id": "mklsqwVt1hCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4 Set the URLs you want to crawl separated by a ,\n",
        "urls = [     \n",
        "    \n",
        "'https://en.wikipedia.org/wiki/Special:Search', 'https://en.wikipedia.org/wiki/Help:Introduction', 'https://en.wikipedia.org/wiki/Special:MyTalk'\n",
        "\n",
        "       ]\n",
        "\n",
        "for url in urls:\n",
        "    # send a GET request to the website\n",
        "    response = requests.get(url)\n"
      ],
      "metadata": {
        "id": "-ZDAWKDV1c2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5 Show the location where keyword is found\n",
        "def search_keyword(urls, keyword):\n",
        "    for url in urls:\n",
        "        page = requests.get(url)\n",
        "        soup = BeautifulSoup(page.content, 'html.parser')\n",
        "        count = 0\n",
        "        for text in soup.find_all(text=True):\n",
        "            if keyword in text:\n",
        "                count +=1\n",
        "        if count:\n",
        "            print(f'Keyword found {count} times at {url}')\n",
        "        \n",
        "search_keyword(urls, 'key word')"
      ],
      "metadata": {
        "id": "PerRXZyT-SnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6 Set the individual URL you want to search\n",
        "url = ['https://en.wikipedia.org/wiki/Special:MyTalk']  \n",
        " \n",
        "for url in url:\n",
        "    # send a GET request to the website\n",
        "    response = requests.get(url)"
      ],
      "metadata": {
        "id": "ivdngJFp-aFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7 Search for specific keywords\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "keywords = ['key word']\n",
        "for keyword in keywords:\n",
        "    matches = soup.find_all(string=lambda text: keyword in text)\n",
        "    print(f'{len(matches)} matches found for keyword \"{keyword}\":')\n",
        "    for match in matches:\n",
        "        print(match)"
      ],
      "metadata": {
        "id": "NDQkBXTRQe_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8 Find a specific 'kind' of tags on the website\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "paragraphs = soup.find_all('p')\n",
        "\n",
        "# Set the keywords you want to search for\n",
        "keywords = ['key word']\n",
        "\n",
        "# Search for the keywords in each paragraph\n",
        "for p in paragraphs:\n",
        "  if any(keyword in p.text for keyword in keywords):\n",
        "    print(p.text)"
      ],
      "metadata": {
        "id": "MZApD2sm8MfX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}